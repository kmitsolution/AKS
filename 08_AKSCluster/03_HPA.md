
# üöÄ **Horizontal Pod Autoscaler (HPA) in Kubernetes / AKS ‚Äî Step-by-Step Guide**

---

## üß© **1Ô∏è‚É£ What is HPA?**

**Horizontal Pod Autoscaler (HPA)** automatically scales the number of pod replicas in a deployment, replica set, or stateful set based on **observed CPU utilization** or **custom metrics**.

* **Scales out (adds pods)** when CPU load increases
* **Scales in (removes pods)** when CPU load decreases
* It **does not** apply to DaemonSets or Jobs

HPA continuously monitors metrics via the **metrics-server**.

---

## ‚öôÔ∏è **2Ô∏è‚É£ Prerequisites**

‚úÖ Kubernetes Cluster (AKS, minikube, etc.)
‚úÖ `metrics-server` installed
‚úÖ `kubectl` configured

### üîπ Check if metrics-server is running:

```bash
kubectl get deployment metrics-server -n kube-system
```

If not present, install it:

```bash
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```

Verify:

```bash
kubectl top nodes
kubectl top pods
```

If you see CPU/memory usage ‚Üí metrics-server works ‚úÖ

---

## üß± **3Ô∏è‚É£ Create a Deployment**

Create file `nginx-deployment.yaml`:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx-app
  template:
    metadata:
      labels:
        app: nginx-app
    spec:
      containers:
      - name: nginx-container
        image: nginx:1.7.9
        resources:
          requests:
            memory: "64Mi"
            cpu: "250m"
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 80
```

Apply it:

```bash
kubectl apply -f nginx-deployment.yaml
```

Check:

```bash
kubectl get deployments
kubectl get pods
```

‚úÖ Output example:

```
NAME            READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deploy    1/1     1            1           1m
```

---

## üåê **4Ô∏è‚É£ Expose the Deployment as a Service**

Create `nginx-service.yaml`:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
  labels:
    app: nginx-app
spec:
  selector:
    app: nginx-app
  type: NodePort
  ports:
  - port: 80
    targetPort: 80
    nodePort: 31111
```

Apply it:

```bash
kubectl apply -f nginx-service.yaml
```

Verify:

```bash
kubectl get svc
```

‚úÖ Output:

```
NAME         TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)        AGE
my-service   NodePort   10.0.12.93    <none>        80:31111/TCP   1m
```

Now your NGINX is exposed at `http://<NodeIP>:31111`.

---

## ‚öôÔ∏è **5Ô∏è‚É£ Create a Horizontal Pod Autoscaler**

You can create it two ways:

### üî∏ Option 1: Command Line

```bash
kubectl autoscale deployment nginx-deploy --cpu-percent=20 --min=1 --max=5
```

### üî∏ Option 2: YAML File

Create `nginx-hpa.yaml`:

```yaml
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: nginx
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-deploy
  minReplicas: 1
  maxReplicas: 10
  targetCPUUtilizationPercentage: 10
```

Apply:

```bash
kubectl apply -f nginx-hpa.yaml
```

---

## üß© **6Ô∏è‚É£ Verify HPA**

```bash
kubectl get hpa
```

‚úÖ Output:

```
NAME     REFERENCE              TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
nginx    Deployment/nginx-deploy  2%/10%     1         10        1          1m
```

* `TARGETS` shows current CPU utilization vs target (10%)
* `REPLICAS` shows how many pods are running

---

## ‚öôÔ∏è **7Ô∏è‚É£ Generate Load to Trigger Autoscaling**

Use a loop to continuously hit the service (simulate traffic):

### On a node or pod inside cluster:

```bash
while true; do wget -q -O- http://localhost:31111; done
```

If accessing externally:

```bash
while true; do curl -s http://<NodeIP>:31111 > /dev/null; done
```

This will increase CPU usage of the pods.

---

## üîç **8Ô∏è‚É£ Monitor Scaling Behavior**

Run:

```bash
kubectl get hpa -w
```

You‚Äôll see something like:

```
NAME     REFERENCE              TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
nginx    Deployment/nginx-deploy  95%/10%     1         10        3         2m
nginx    Deployment/nginx-deploy  115%/10%    1         10        5         3m
```

Now check the deployment:

```bash
kubectl get deployment nginx-deploy
```

‚úÖ Output:

```
NAME            READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deploy    5/5     5            5           5m
```

The number of pods scaled up automatically üöÄ

---

## ‚öôÔ∏è **9Ô∏è‚É£ Scale Back Down**

When you stop the load test, CPU utilization drops.
After a few minutes, HPA automatically scales pods down.

Check again:

```bash
kubectl get hpa
kubectl get pods
```

‚úÖ You‚Äôll see replicas return to 1 or 2.

---

## üßæ **10Ô∏è‚É£ Clean Up**

When done:

```bash
kubectl delete -f nginx-hpa.yaml
kubectl delete -f nginx-service.yaml
kubectl delete -f nginx-deployment.yaml
```

---

# üìò **Summary**

| Step | Command / File                           | Description             |
| ---- | ---------------------------------------- | ----------------------- |
| 1    | `kubectl apply -f nginx-deployment.yaml` | Deploy nginx            |
| 2    | `kubectl apply -f nginx-service.yaml`    | Expose deployment       |
| 3    | `kubectl apply -f nginx-hpa.yaml`        | Create autoscaler       |
| 4    | `kubectl get hpa`                        | Check autoscaler status |
| 5    | `while true; do wget ...; done`          | Generate load           |
| 6    | `kubectl get hpa -w`                     | Watch scaling           |
| 7    | Stop load                                | Observe scale down      |

---

# üß† **Key Points**

‚úÖ HPA requires `metrics-server`
‚úÖ Uses CPU utilization % (default metric)
‚úÖ Works with Deployments, ReplicaSets, StatefulSets
‚úÖ Automatically adjusts replica count between min/max
‚úÖ Great for cost optimization and handling variable traffic

---

